{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 3.5 SonnetSolution\n",
    "\n",
    "This solution is based on the XGBoost solution in the seperate file. Claude 3.5 Sonnet was then asked to generate a PyTorch version that performs equivalently. \n",
    "\n",
    "It's worth noting that the submission below doesn't perform quite as well as XGBoost, which is evidently similar to what is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olivernormand/GitHub/veritas/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val Loss: 0.2342\n",
      "Epoch 10, Val Loss: 0.1723\n",
      "Epoch 20, Val Loss: 0.1375\n",
      "Epoch 30, Val Loss: 0.1283\n",
      "Early stopping at epoch 38\n",
      "\n",
      "Validation Results:\n",
      "   y_val_actual  y_pred_actual\n",
      "0      154500.0  148498.890625\n",
      "1      325000.0  321701.781250\n",
      "2      115000.0  109539.359375\n",
      "3      159000.0  173064.656250\n",
      "4      315500.0  366053.593750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/j9hypd8x28q8j95_jd0b_mlm0000gn/T/ipykernel_9141/3618232270.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define the network\n",
    "class HousePriceNet(nn.Module):\n",
    "    def __init__(self, input_size=79):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(torch.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm2(torch.relu(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm3(torch.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Data loading and preprocessing (same as before)\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_concat = pd.concat([df_train, df_test])\n",
    "\n",
    "# Preprocessing (similar to XGBoost version)\n",
    "X = df_concat.drop([\"SalePrice\", \"Id\"], axis=1)\n",
    "y = df_concat[\"SalePrice\"]\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_columns = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "# Label encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    X[col] = X[col].fillna(\"missing\")\n",
    "    X[col] = label_encoders[col].fit_transform(X[col])\n",
    "\n",
    "# Handle numerical missing values\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X[numerical_columns] = imputer.fit_transform(X[numerical_columns])\n",
    "\n",
    "# Scale features and target\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X = pd.DataFrame(X_scaler.fit_transform(X), columns=X.columns)\n",
    "y = pd.Series(y_scaler.fit_transform(y.values.reshape(-1, 1)).ravel())\n",
    "\n",
    "# Split back into train/test\n",
    "X, X_test = X.iloc[: df_train.shape[0]], X.iloc[df_train.shape[0] :]\n",
    "y, y_test = y.iloc[: df_train.shape[0]], y.iloc[df_train.shape[0] :]\n",
    "\n",
    "# Create train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "y_train = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "X_val = torch.FloatTensor(X_val.values)\n",
    "y_val = torch.FloatTensor(y_val.values).reshape(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "\n",
    "# Training setup\n",
    "model = HousePriceNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i : i + batch_size]\n",
    "        batch_y = y_train[i : i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Load best model and make predictions\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(X_val)\n",
    "    y_test_pred = model(X_test)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_val_actual = y_scaler.inverse_transform(y_val.numpy())\n",
    "y_val_pred_actual = y_scaler.inverse_transform(y_val_pred.numpy())\n",
    "y_submission = y_scaler.inverse_transform(y_test_pred.numpy())\n",
    "\n",
    "# Create submission file\n",
    "df_submission = pd.DataFrame({\"Id\": df_test[\"Id\"], \"SalePrice\": y_submission[:, 0]})\n",
    "df_submission.to_csv(\"submission_torch.csv\", index=False)\n",
    "\n",
    "# Print validation results\n",
    "df = pd.DataFrame(\n",
    "    {\"y_val_actual\": y_val_actual[:, 0], \"y_pred_actual\": y_val_pred_actual[:, 0]}\n",
    ")\n",
    "print(\"\\nValidation Results:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
